{
 "metadata": {
  "name": "",
  "signature": "sha256:a37e161c60bab5608b2d71640da144650c62622c2925f3700ff30e9eced5f7d6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# CSE 4334/5334 Programming Assignment 1 (P1)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Spring 2016"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Due: 11:59pm Central Time, Thursday, March 3, 2016"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this assignment, you will implement a toy \"search engine\". More specifically, you code will read a corpus and produce TF-IDF vectors for documents in the corpus. Then, given a query string, you code will compute the cosine similarity between the query vector and the document vectors and return the document that gets the highest similarity score. \n",
      "\n",
      "The instructions on this assignment are written in an .ipynb file. You can use the following commands to install the iPython notebook viewer:\n",
      "\n",
      "    pip install ipython\n",
      "\n",
      "    pip install notebook (You might have to use sudo if you are installing them at system level)\n",
      "\n",
      "To run the iPython notebook viewer, use the following command\n",
      "\n",
      "    ipython notebook P1.ipynb\n",
      "\n",
      "The above command will start a webservice at http://localhost:8888/ and display a '.ipynb' file."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Requirements"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* This assignment must be done individually. You must implement the whole assignment by yourself. Academic dishonety will have serious consequences.\n",
      "* You are encouraged to discuss the assignment with other students, but you are not allowed to disclose your solution. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use a corpus of all the general election presidential debates from 1960 to 2012. We processed the corpus and provided you a .zip file, which includes 30 .txt files. Each of the 30 files contains the transcript of a debate and is named by the date of the debate. The .zip file can be downloaded from Blackboard (\"Course Materials\" > \"Programming Assignment 1\" > \"Attached Files: presidential_debates.zip\")."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Programming Language"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. You are required to use Python 3.5.1. You are required to submit a single .py file of your code. We will test your code under the particular version of Python 3.5.1. So make sure you develop your code using the same version. \n",
      "\n",
      "2. You are expected to use several modules in NLTK--a natural language processing toolkit for Python. NLTK doesn't come with Python by default. You need to install it and \"import\" it in your .py file. NLTK's website (http://www.nltk.org/index.html) provides a lot of useful information, including a book http://www.nltk.org/book/, as well as installation instructions (http://www.nltk.org/install.html).\n",
      "\n",
      "3. In programming assignment 1, other than NLTK, you are not allowed to use any other non-standard Python package. However, you are free to use anything from the the Python Standard Library that comes with Python 3.5.1 (https://docs.python.org/3/library/)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Tasks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You code should accomplish the following tasks:\n",
      "\n",
      "(1) <b>Read</b> the 30 .txt files, each of which has the transcript of a presidential debate. The following code does it. Make sure to replace corpus_root by your directory where the files are stored.\n",
      "\n",
      "Also, convert the text to lower case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "    \n",
      "corpus_root = './presidential_debates'\n",
      "for filename in os.listdir(corpus_root):\n",
      "    file = open(os.path.join(corpus_root, filename), \"r\")\n",
      "    doc = file.read()\n",
      "    file.close() \n",
      "    doc = doc.lower()"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(2) <b>Tokenize</b> the content of each file. For this, you need a tokenizer. For example, the following piece of code uses a regular expression tokenizer to return all course numbers in a string. Play with it and edit it. You can change the regular expression and the string to observe different output results. \n",
      "\n",
      "For tokenizing the Presidential debate speeches, let's all use RegexpTokenizer(r'[a-zA-Z]+'). What tokens will it produce? What limitations does it have? \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer\n",
      "\n",
      "tokenizer = RegexpTokenizer(r'[A-Z]{2,3}[1-9][0-9]{3,3}')\n",
      "tokens = tokenizer.tokenize(\"CSE4334 and CSE5334 are taught together. IE3013 is an undergraduate course.\")\n",
      "print(tokens)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['CSE4334', 'CSE5334', 'IE3013']\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(3) Perform <b>stopword removal</b> on the obtained tokens. NLTK already comes with a stopword list, as a corpus in the \"NLTK Data\" (http://www.nltk.org/nltk_data/). You need to install this corpus. Follow the instructions at http://www.nltk.org/data.html. You can also find the instruction in this book: http://www.nltk.org/book/ch01.html (Section 1.2 Getting Started with NLTK). Basically, use the following statements in Python interpreter. A pop-up window will appear. Click \"Corpora\" and choose \"stopwords\" from the list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After the stopword list is downloaded, you will find a file \"english\" in folder nltk_data/corpora/stopwords, where folder nltk_data is the download directory in the step above. The file contains 127 stopwords. nltk.corpus.stopwords will give you this list of stopwords. Try the following piece of code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "    print(stopwords.words('english'))\n",
      "    print(sorted(stopwords.words('english')))"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
        "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'did', 'do', 'does', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'itself', 'just', 'me', 'more', 'most', 'my', 'myself', 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 's', 'same', 'she', 'should', 'so', 'some', 'such', 't', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(4) Also perform <b>stemming</b> on the obtained tokens. NLTK comes with a Porter stemmer. Try the following code and learn how to use the stemmer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.porter import PorterStemmer\n",
      "    stemmer = PorterStemmer()\n",
      "    stemmer.stem('studying')"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "text/plain": [
         "'studi'"
        ]
       },
       "execution_count": 36,
       "metadata": {},
       "output_type": "execute_result"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(5) Using the tokens, compute the <b>TF-IDF vector</b> for each document. Use the following equation that we learned in the lectures to calculate the term weights:  $$(1+log_{10}{tf_{t,d}})\\times(log_{10}{\\frac{N}{df_t}}).$$ Note that the TF-IDF vectors should be normalized (i.e., their lengths should be 1). \n",
      "\n",
      "Represent a TF-IDF vector by a dictionary. The following is a sample TF-IDF vector."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "{\n",
      "    'sanction': 0.014972337775895645, \n",
      "    'lack': 0.008576372825970286, \n",
      "    'regret': 0.009491784747267843, \n",
      "    'winter': 0.030424375278541155\n",
      "}"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "{'lack': 0.008576372825970286,\n",
        " 'regret': 0.009491784747267843,\n",
        " 'sanction': 0.014972337775895645,\n",
        " 'winter': 0.030424375278541155}"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(6) Given a query string, calculate the query vector. Compute the <b>cosine similarity</b> between the query vector and every document vector. Return the document that attains the highest cosine similarity score.\n",
      "\n",
      "In calculating the query vector, don't consider IDF. I.e., use the following equation to calculate the term weights in the query vector: $$(1+log_{10}{tf_{t,d}}).$$\n",
      "The vector should still be normalized. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What to Submit "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Submit through Blackboard your source code in a single .py file. You can use any standard Python library. The only non-standard library/package allowed for this assignment is NLTK.\n",
      "\n",
      "The file must define at least the following functions:\n",
      "\n",
      "* query(qstring): return the document that has the highest similarity score with respect to 'qstring'.\n",
      "\n",
      "* getcount(token): return the total number of occurrences of a token in all documents.\n",
      "\n",
      "* getidf(token): return the inverse document frequency of a token. If the token doesn't exist in the corpus, return 0. \n",
      "\n",
      "* docdocsim(filename1,filename2): return the cosine similarity betwen two speeches (files).\n",
      "\n",
      "* querydocsim(qstring,filename): return the cosine similairty between a query string and a document. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some sample results that we should expect from a correct implementation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* print(query(\"health insurance wall street\"))\n",
      "\n",
      "2012-10-03.txt\n",
      "\n",
      "* print(getcount(\"health\"))\n",
      "\n",
      "483\n",
      "\n",
      "* print(\"%.12f\" % getidf(\"health\"))\n",
      "\n",
      "0.079181246048\n",
      "\n",
      "* print(\"%.12f\" % docdocsim(\"1960-09-26.txt\", \"1980-09-21.txt\"))\n",
      "\n",
      "0.068531196532\n",
      "\n",
      "* print(\"%.12f\" % querydocsim(\"health insurance wall street\", \"1996-10-06.txt\"))\n",
      "\n",
      "0.022032578028"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will evaluate your code by calling the functions specificed above. So, make sure to use the same function names, parameter names/types/orders as specified above. "
     ]
    }
   ],
   "metadata": {
    "kernelspec": {
     "display_name": "Python 3",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.5.1"
    }
   }
  }
 ]
}